"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[2073],{446:e=>{e.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"matrices-github-actions","metadata":{"permalink":"/blog/tech/matrices-github-actions","source":"@site/tech/2025-03-28-matrices-github-actions/index.md","title":"An Ultimate Guide to Matrices in GitHub Actions, from Basic to Advanced","description":"Listen, I feel the boldest of girls for calling this *an* ultimate guide. But it\'s just because this will be the one post about matrices I\'ll update over time as I experience more. All info in a single place for easier retrieval, enjoy!","date":"2025-03-28T00:00:00.000Z","tags":[{"label":"github actions","permalink":"/blog/tech/tags/github-actions"},{"label":"ci/cd","permalink":"/blog/tech/tags/ci-cd"},{"label":"devops","permalink":"/blog/tech/tags/devops"}],"readingTime":17.47,"hasTruncateMarker":true,"authors":[{"name":"Manu Magalh\xe3es","title":"DevSecOps Engineer","url":"https://github.com/magmanu","imageURL":"https://github.com/magmanu.png","key":"manu"}],"frontMatter":{"slug":"matrices-github-actions","title":"An Ultimate Guide to Matrices in GitHub Actions, from Basic to Advanced","authors":"manu","tags":["github actions","ci/cd","devops"]},"unlisted":false,"nextItem":{"title":"GitHub Actions: Data Flow & Data Persistence","permalink":"/blog/tech/github-actions-data-flow"}},"content":"_Listen, I feel the boldest of girls for calling this *an* ultimate guide. But it\'s just because this will be the one post about matrices I\'ll update over time as I experience more. All info in a single place for easier retrieval, enjoy!_\\n\\nMatrices keep workflows DRY: you setup a job once and let the matrix do the boring work of multiplying it with all the different configurations required.  \\n\x3c!--truncate--\x3e\\n\\n## Basics: Simple Matrix\\n\\nUse a matrix to run the same job multiple times, with different configurations.  \\n\\nMatrices are set at the job level, under the `strategy` key. Then, at the step level, reference the matrix values with `${{ matrix.key_name}}`.\\n\\nLet\'s follow the classic workflow example, of course: running CI tests in different versions of Node.js.\\n\\n<details>\\n<summary>\\n  Show code\\n</summary>\\n<div>\\n\\n```yaml title=\\".github/workflows/testing.yaml\\" showLineNumbers\\n(...)\\njobs:\\n  unit-tests:\\n    // highlight-start\\n    strategy:\\n      matrix:\\n        # I named it `node-version`, but you can name it however you want\\n        node-version: [16, 18] \\n    // highlight-end\\n    runs-on: ubuntu-latest\\n    steps:\\n      - name: Checkout\\n        (...)\\n\\n      // highlight-start\\n      # And to reference it, use `${{ matrix.key_name}}`\\n      - name: Use Node.js ${{ matrix.node-version }}\\n      // highlight-end\\n        uses: actions/setup-node@v4\\n        with:\\n        // highlight-next-line\\n          node-version: ${{ matrix.node-version }}\\n\\n      (...)\\n  ```\\n\\n</div>\\n</details>\\n\\n\\n<details>\\n<summary>\\n  Show pipeline\\n</summary>\\n<div>\\n\\n![screenshot](./screenshot_13.png)\\n</div>\\n</details>\\n\\n## Multidimensional Matrices (Matrix with Multiple Keys)\\n\\nOf course this can get more complex. If you want to run your tests in Node.js 16 & 18, **and** test each node version in a different OS, you can use a matrix with multiple keys (aka matrix of matrices, or multidimensional matrices).\\n\\nPlease note: in a matrix of matrices, the number of jobs is the product of the matrix sizes - that is, the number of items in matrix A multiplied by the number of items in matrix B. \\n\\nSo if we specify two node versions and two OSs, like this...\\n\\n```yaml title=\\".github/workflows/testing.yaml\\" showLineNumbers\\nmatrix:\\n  os: [ubuntu-latest, windows-latest]\\n  node-version: [16, 18]\\n```\\n\\n... we\'ll end up with 4 jobs (2*2):\\n\\n- ubuntu-latest + node 16\\n- ubuntu-latest + node 18\\n- windows-latest + node 16\\n- windows-latest + node 18\\n\\n<details>\\n<summary>\\n  Show code\\n</summary>\\n\\n<div>\\n\\n```yaml title=\\".github/workflows/testing.yaml\\" showLineNumbers\\n(...)\\n  unit-tests:\\n    strategy:\\n      matrix:\\n      // highlight-start\\n        os: [ubuntu-latest, windows-latest]\\n        node-version: [16, 18]\\n    runs-on: ${{ matrix.os }}\\n    // highlight-end\\n    steps:\\n      - name: Checkout\\n        (...)\\n\\n      // highlight-next-line\\n      - name: Use Node.js ${{ matrix.node-version }} in ${{ matrix.os }}\\n        uses: actions/setup-node@v2\\n        with:\\n          node-version: ${{ matrix.node-version }}\\n\\n      (...)\\n  ```\\n\\n</div>\\n</details>\\n\\n<details>\\n<summary>\\n  Show pipeline\\n</summary>\\n<div>\\n\\n![screenshot](./screenshot_14.png)\\n</div>\\n</details>\\n\\nYou can have as many keys you want, but keep in mind that the number of jobs **can grow exponentially**. For example, if you have 4 matrices keys with 3 values each, you\'ll end up with 81 jobs (3\\\\*3\\\\*3\\\\*3).\\n\\n## Adjusting matrices with `include` & `exclude`, part I: the \\"odd job\\"\\n\\n### Static usage\\nIf you know beforehand all the matrix values you\'ll use, you can set them statically.  \\n\\nLet\'s say your team will also run tests for Node.js 18 in Windows machines. There are two ways to handle this:\\n\\n#### `include`\\nWith `include`, you can **add** an \\"odd value\\" to your matrix.  \\nBelow, we\'ll write a matrix for combinations between the Ubuntu OS and two Node.js versions, and use `include` to add the odd `windows-latest` + Node.js `18` combination.\\n\\n<details>\\n<summary>\\n  Show code\\n</summary>\\n\\n<div>\\n\\n```yaml title=\\".github/workflows/testing.yaml\\" showLineNumbers\\n(...)\\n  unit-tests:\\n    strategy:\\n      matrix:\\n        os: [ubuntu-latest]\\n        node-version: [16, 18]\\n      // highlight-start\\n      include: # add the odd combination\\n        - os: windows-latest\\n          node-version: 18\\n      // highlight-end\\n    runs-on: ${{ matrix.os }}\\n        (...)\\n  ```\\n\\n</div>\\n</details>\\n\\n#### `exclude`\\n`exclude` works in the opposite direction: you add all values to the matrix and use `exclude` to **remove** the undesirable matches.\\n\\n<details>\\n<summary>\\n  Show code\\n</summary>\\n<div>\\n\\n```yaml title=\\".github/workflows/testing.yaml\\" showLineNumbers\\n(...)\\n  unit-tests:\\n    strategy:\\n      matrix:\\n        os: [ubuntu-latest, windows-latest]\\n        node-version: [16, 18]\\n      // highlight-start\\n      exclude: # remove unwanted combination\\n        - os: windows-latest\\n          node-version: 16\\n      // highlight-end\\n    runs-on: ${{ matrix.os }}\\n        (...)\\n  ```\\n\\n</div>\\n</details>\\n\\n<details>\\n<summary>\\n  Show pipeline: the pipeline looks the same in both cases\\n</summary>\\n<div>\\n\\nNow there are only 3 jobs - also, the job completion for windows went down from 1m23/1m35s to 44s thanks to caching :)\\n\\n![screenshot](./screenshot_15.png)\\n</div>\\n</details>\\n\\n\\n### The Declarative Case: `include` as a flattenning tool\\n\\nIf you prefer clarity over brevity, you can use `include` to flatten matrices. Just beware that this can make your workflow harder to maintain.\\n\\n<details>\\n<summary>\\n  Show code\\n</summary>\\n\\n<div>\\n\\n![screenshot](./screenshot_1.png)\\n</div>\\n</details>\\n\\n### Dynamic exclusions\\n\\nThis is my favourite. You can use `exclude` to dynamically adjust matrix values. \\n\\nLet\'s suppose your team decides that every Tuesday you will run tests in the most recent Node.js version, as a peek into changes you\'ll have to make in the future. You can use `include` to run the new version to the matrix *only on Tuesdays*.\\n\\n<details>\\n<summary>\\n  Show code\\n</summary>\\n\\n<div>\\n\\n```yaml title=\\".github/workflows/testing.yaml\\" showLineNumbers\\n  jobs:\\n    get_weekday:\\n      runs-on: ubuntu-latest\\n      outputs:\\n        WEEKDAY: ${{ steps.get_weekday.outputs.WEEKDAY }}\\n      steps:\\n        - name: Get day of the week\\n          id: get_weekday\\n          run: echo \\"WEEKDAY=$(date \'+%a\')\\" | tee -a \\"$GITHUB_OUTPUT\\"\\n\\n    unit-tests:\\n      runs-on: ubuntu-latest\\n      needs: get_weekday\\n      strategy:\\n        matrix:\\n          node-version: [16, 18, 24]\\n          // highlight-start\\n          exclude:\\n            - node-version: ${{ needs.get_weekday.outputs.WEEKDAY != \'Tue\' && 24 || \'\' }}\\n          // highlight-end\\n      steps:\\n        - name: Run unit tests\\n          run: |\\n            echo \\"Today is ${{ needs.get_weekday.outputs.WEEKDAY }}.\\"\\n            echo \\"Let\'s run tests for Node.js v${{ matrix.node-version }}\\"\\n      (...)\\n  ```\\n</div>\\n</details>\\n\\n<details>\\n<summary>\\n  Show pipeline\\n</summary>\\n<div>\\n\\n![screenshot](./screenshot_2.png)\\n</div>\\n</details>\\n\\nYou may have noticed this funny syntax: `${{ needs.get_weekday.outputs.WEEKDAY != \'Tue\' && 24 || \'\' }}`. This is a way to [use ternaries in Github Actions](https://docs.github.com/en/actions/learn-github-actions/expressions#:~:text=ternary,expressions). If the weekday *is not* Tuesday, this expression will return `24`, meaning that `24` will be excluded from the `node-version` array values. But, on Tuesdays, this expression will return an empty value => `24` will not be excluded =>  all node-versions set in the base matrix run => Node.js 24 will run.\\n\\nNow, why `exclude` instead of `include`?  \\nHonestly, I haven\'t found a decent way to get `included` to work with this use case. With `include`, the syntax would be dreadful:  We have to use a ternary both for key and value. You sure you want... this thing below? I sure don\'t.\\n\\n```yaml\\ninclude:\\n  - { \\"${{needs.get_weekday.outputs.WEEKDAY != \'Tue\' && \'node-version\' || \'\'}}\\": \\"${{needs.get_weekday.outputs.WEEKDAY != \'Tue\' && 24 || \'\'}}\\" }\\n```\\n\\n## Adjusting matrices with `include` & `exclude`, part II: the small print\\n\\n### What `include` doesn\'t do\\n\\nFirst, let\'s see what you **cannot/should not** do with `include`:\\n\\n#### `include` doesn\'t inherit the matrix key/value pairs\\n\\nJust because a key/value pair is present in the matrix, it doesn\'t mean `include` inherits it.  \\n\\n```yaml\\n    strategy:\\n        matrix:\\n          os: [windows-latest, ubuntu-latest]\\n          node-version: [18]\\n          region: [Asia, Latam]\\n          // highlight-start\\n          include:\\n            - os: macos-latest\\n          // highlight-end\\n    steps:\\n      - name: ${{ matrix.os }} ${{ matrix.node-version }} ${{ matrix.region }}\\n        run: |\\n          echo \\"OS: ${{ matrix.os }}\\"\\n          echo \\"Node Version: ${{ matrix.node-version }}\\"\\n          echo \\"Region: ${{ matrix.region }}\\"\\n```\\nIn this example, because you haven\'t defined a `node-version` or `region` in your include map, those values will be empty in the included job.\\n\\n\\n<details>\\n<summary>\\n  Show pipeline\\n</summary>\\n<div>\\n\\n![screenshot](./screenshot_16.png)\\n</div>\\n</details>\\n\\n#### Array values for `include` do not spread into separate jobs\\n\\nIn this example, the `region` array  **will not** spin a `macos-latest` job for each region. The `include` will spin a **single** MacOS job that takes the whole array as the value for `region`, as you can see in the screenshot below. You can, however, extract the values from the array using the bracket notation `matrix.region[0]`.\\n\\n```yaml\\n    strategy:\\n        matrix:\\n          os: [windows-latest, ubuntu-latest]\\n          node-version: [18]\\n          region: [Asia, Latam]\\n          include:\\n            - os: macos-latest\\n            // highlight-next-line\\n              region: [Asia, Latam]\\n    steps:\\n      - name: ${{ matrix.os }} ${{ matrix.region }}\\n        run: |\\n          echo \\"OS: ${{ matrix.os }}\\"\\n          echo \\"Node Version: ${{ matrix.node-version }}\\"\\n          echo \\"First Region: ${{ matrix.region[0] }}\\"\\n          echo \\"Second Region: ${{ matrix.region[1] }}\\"\\n```\\n<details>\\n<summary>\\n  Show pipeline\\n</summary>\\n<div>\\n\\n![screenshot](./screenshot_17.png)\\n</div>\\n</details>\\n\\n### Advice for using`include`\\n\\nNow, what you **can/must** do with `include`, some of which you might have guessed already from the previous points.  \\n\\n#### Be explicit about all desired key/value pairs to the matrix\\n\\nIf you want a key/value pair in `include`, well... include it. It won\'t magically appear by inheritance.\\n\\n```yaml\\n    strategy:\\n        matrix:\\n          os: [windows-latest, ubuntu-latest]\\n          node-version: [18]\\n          region: [Asia, Latam]\\n          // highlight-start\\n          include:\\n            - os: macos-latest\\n              node-version: 20\\n              region: Asia\\n          // highlight-end\\n    steps:\\n      - name: ${{ matrix.os }} ${{ matrix.node-version }} ${{ matrix.region }}\\n        run: |\\n          echo \\"OS: ${{ matrix.os }}\\"\\n          echo \\"Node Version: ${{ matrix.node-version }}\\"\\n          echo \\"Region: ${{ matrix.region }}\\"\\n```\\n\\n#### Each `include` map is an independent unit\\n\\nIn the previous example, the `macos-latest` job was only added to the `Asia` region. \\nIf you want  `macos-latest` for `Asia` and `Latam`, you need to add an `include` map for each of them. Do not try to combine them in a single map, that\'s not how `include` works.\\n\\n```yaml\\n    strategy:\\n        matrix:\\n          os: [windows-latest, ubuntu-latest]\\n          node-version: [18]\\n          region: [Asia, Latam]\\n          // highlight-start\\n          include:\\n            - os: macos-latest\\n              node-version: 20\\n              region: Asia\\n            - os: macos-latest\\n              node-version: 20\\n              region: Latam\\n          // highlight-end\\n    steps:\\n      - name: ${{ matrix.os }} ${{ matrix.node-version }} ${{ matrix.region }}\\n        run: |\\n          echo \\"OS: ${{ matrix.os }}\\"\\n          echo \\"Node Version: ${{ matrix.node-version }}\\"\\n          echo \\"Region: ${{ matrix.region }}\\"\\n```\\n\\n#### Feel free to add brand new keys not present in the base matrix\\n\\nYou\'re not restricted to modifying keys that already exist in the base matrix. You can add new ones too. Just keep in mind that \\"new\\" keys will be added to **all* jobs in the matrix.  \\n\\n```yaml\\n    strategy:\\n        matrix:\\n          os: [windows-latest, ubuntu-latest]\\n          node-version: [18]\\n          region: [Asia, Latam]\\n          // highlight-start\\n          include:\\n            - report: true\\n          // highlight-end\\n    steps:\\n      - name: ${{ matrix.os }} ${{ matrix.node-version }} ${{ matrix.region }} ${{ matrix.report }}\\n        run: |\\n          echo \\"OS: ${{ matrix.os }}\\"\\n          echo \\"Node Version: ${{ matrix.node-version }}\\"\\n          echo \\"Region: ${{ matrix.region }}\\"\\n          echo \\"Should report: ${{ matrix.report }}\\"\\n```\\n\\n<details>\\n<summary>\\n  Show pipeline\\n</summary>\\n<div>\\nRepating, just to highlight it:  \\n- When the keys of an `include` map don\'t match any keys in the base matrix (as in the example above), **that new key will be added to every job in the matrix**.  \\n\\n![screenshot](./screenshot_18.png)\\n</div>\\n</details>\\n\\n\\n## Case Study: More Complex Matrices\\n\\nWhen real life complexity knocks at your door, `include` and `exclude` can become confusing. I\'ll walk you through an implementation example to help you reason through your options.\\n\\nLet\'s take the following scenario: you need to run tests for your product, but different regions have different requirements for the test suite. But they have one thing in common: both regions require that you produce a report for all tests. Here are your specs:\\n\\nAsia Region:\\n- run tests for Node.js 18 in Ubuntu and Windows\\n- run tests for Node.js 20 in Ubuntu and MacOs \\n\\nLatam Region:\\n- run tests for Node.js 18 in Ubuntu and Windows\\n- run tests for Node.js 20 in Ubuntu only\\n- run tests for Node.js 22 in Windows, Ubuntu and MacOs\\n\\nExport test report:\\n- Adds a variable to the matrix to indicate if the test report should be exported. Your requirement is that all tests should output a report.\\n\\nFor clarity, I\'ll spell out what jobs will run for each product:\\n\\n| Config | Windows | Ubuntu | MacOs |\\n| --- | --- | --- | --- |\\n| Region: Asia | 18 | 18, 20 | 20 |\\n| Region: Latam | 18, 22 | 18, 20, 22 | 22 |\\n\\n\\n### The declarative approach\\n\\nWith this approach, you make each combination very explicit. It\'s quite easy to see how this can get a bit out of hand as your test suite grows.\\n\\nBut there\'s a silly trick to make it less awful to read: just tweak the `include` syntax using curly brackets. This is my favourite notation for matrices, as the visual pattern makes the reading more manageable.\\n\\n<details>\\n<summary>\\n  Show code\\n</summary>\\n\\n<div>\\n\\n```yaml title=\\".github/workflows/testing.yaml\\" showLineNumbers\\n(...)\\n  unit-tests:\\n    strategy:\\n      matrix:\\n      include:\\n        - {os: windows-latest, node-version: 18, region: Asia, report: true}\\n        - {os: windows-latest, node-version: 18, region: Latam, report: true}\\n        - {os: windows-latest, node-version: 22, region: Latam, report: true}\\n        - {os: ubuntu-latest, node-version: 18, region: Asia, report: true}\\n        - {os: ubuntu-latest, node-version: 18, region: Latam, report: true}\\n        - {os: ubuntu-latest, node-version: 20, region: Asia, report: true}\\n        - {os: ubuntu-latest, node-version: 20, region: Latam, report: true}\\n        - {os: ubuntu-latest, node-version: 22, region: Latam, report: true}\\n        - {os: macos-latest, node-version: 20, region: Asia, report: true}\\n        - {os: macos-latest, node-version: 22, region: Latam, report: true}\\n    runs-on: ${{ matrix.os }}\\n    steps:\\n      - name: Run tests\\n        run: | \\n          echo \\"Running tests for Node.js ${{ matrix.node-version }} in ${{ matrix.os }} for product ${{ matrix.product }}\\"\\n          echo \\"Should report: ${{ matrix.report }}\\"\\n      (...)\\n  ```\\n</div>\\n</details>\\n \\n### The `include`/`exclude` approach\\n\\nSome people might prefer this approach, but it requires a little training to tell what specific jobs will be actually spinned up.\\n\\nIn this example, we will build our base matrix with all applicable values: \\n\\n```yaml title=\\".github/workflows/testing.yaml\\"\\n  matrix:\\n    os: [windows-latest, ubuntu-latest, macos-latest]\\n    node-version: [18, 20, 22]\\n    region: [Asia, Latam]\\n```\\n\\n_As an exercise, how many jobs do we have there?_\\n```\\nos_length (3) * node-version_length (3) * region_length (2) = 18\\n```\\n\\nThen we can proceed to trim it down to the desired cases only.  \\nKeep in mind that `include` is processed after `exclude`, so we can use `include` to add back combinations that were previously excluded.\\n\\n```yaml \\n  matrix:\\n    os: [windows-latest, ubuntu-latest, macos-latest]\\n    node-version: [18, 20, 22]\\n    region: [Asia, Latam]\\n    exclude:\\n      - {node-version: 22, region: Asia}                        # Asia has no case for Node.js 22 (-3 jobs)\\n      - {node-version: 18, os: macos-latest}                    # MacOs won\'t test Node.js 18 (-2 jobs)\\n      - {node-version: 20, region: Asia, os: windows-latest}    # Windows is the only case without Node.js 20 in Asia (-1 job)\\n      - {node-version: 20, region: Latam}                       # There\'s only one case for Node 20 in Latam, we\'ll readd it later (-3 jobs)\\n    include:\\n      - {node-version: 20, region: Latam, os: ubuntu-latest}    # Re-adding the case for Node 20 in Latam (+1 job)\\n```\\n\\n<details>\\n<summary>\\n  Show pipeline\\n</summary>\\n<div>\\n\\n![screenshot](./screenshot_19.png)\\n</div>\\n</details>\\n\\n\\nAnd finally, because the report should be added to all cases, we can add `report` under `include`:\\n\\n```yaml \\n  matrix:\\n    os: [windows-latest, ubuntu-latest, macos-latest]\\n    node-version: [18, 20, 22]\\n    region: [Asia, Latam]\\n    exclude:\\n      - {node-version: 22, region: Asia}\\n      - {node-version: 18, os: macos-latest}\\n      - {node-version: 20, region: Asia, os: windows-latest}\\n      - {node-version: 20, region: Latam}\\n    include:\\n      - {node-version: 20, region: Latam, os: ubuntu-latest}\\n      // highlight-start\\n      # doesn\'t add new jobs, just adds this key to all jobs\\n      - report: true \\n      // highlight-end\\n```\\n\\n<details>\\n<summary>\\n  Show pipeline\\n</summary>\\n<div>\\n\\n![screenshot](./screenshot_20.png)\\n</div>\\n</details>\\n\\nAnd that\'s how we end with our 10 desired jobs.\\n\\n## Simplifying \\"particular combinations\\" in matrices\\n\\nIf you\'re using multidimensional matrices, the more configurations you set, the more difficult it is to handle outliers. So here\'s a trick I learned from [Sean Killeen\'s blog](https://seankilleen.com/2023/08/how-to-specify-pairs-of-items-in-github-actions-matrix-strategies/): instead of using `include`, `exclude` and whatnot, you can use maps to easily group key/value pairs that usually go together but don\'t quite fit a matrix context because they are too tighly coupled. It won\'t work for all cases, but will be a good solution for plenty of them:\\n\\n\\n```yaml title=\\".github/workflows/sean.yaml\\" showLineNumbers\\njobs:\\n  build_release:\\n    name: \\"Build and Release\\"\\n    strategy:\\n      matrix:\\n        // highlight-next-line\\n        VERSIONS: [ {ruby: 2.7.3, ghpages: 226}, {ruby: 2.7.4, ghpages: 228}] # tight coupling between rub and ghpages versions, not very \\"matrixy\\"\\n        NODE_MAJOR_VERSION: [16,18,20]\\n    runs-on: ubuntu-latest\\n    env:\\n      NODE_MAJOR_VERSION: ${{ matrix.NODE_MAJOR_VERSION }}\\n      // highlight-start\\n      RUBY_VERSION: ${{ matrix.VERSIONS.ruby }}\\n      GITHUB_PAGES_VERSION: ${{ matrix.VERSIONS.ghpages }}\\n      // highlight-end\\n```\\n\\n\x3c!-- \\n## TODO: Generating Your Own Matrix\\n\\nA matrix, in itself, is nothing more than a JSON. \\n\\n```json title=\\"matrix.json\\" { \\"props\\": { \\"style\\": { \\"maxHeight\\": \\"300px\\" } } }\\n{\\n  \\"include\\": [\\n    {\\n      \\"project\\": \\"foo\\",\\n      \\"config\\": \\"Debug\\"\\n    },\\n    {\\n      \\"project\\": \\"bar\\",\\n      \\"config\\": \\"Release\\"\\n    }\\n  ]\\n}\\n```\\n\\njobs:\\n  prepare:\\n    runs-on: ubuntu-latest\\n    outputs:\\n      repository: ${{ steps.json.outputs.repository }}\\n    steps:\\n      - name: build matrix\\n        id: json\\n        run: |\\n          repository=\'{ \\"repository\\": [\\"repo1\\",\\"repo2\\",\\"repo3\\",\\"repo4\\"] }\'\\n          echo \\"repository=$repository\\" >> \\"$GITHUB_OUTPUT\\"\\n\\n  run-matrix:\\n    needs: prepare\\n    runs-on: ubuntu-latest\\n    strategy:\\n      fail-fast: false\\n      matrix: ${{ fromJson(needs.prepare.outputs.repository) }}\\n    steps:\\n      - run: echo \\"${{ matrix.repository }}\\" --\x3e --\x3e\\n\\n\x3c!-- \\n## TODO: Resource Management: Max Number of Concurrent Jobs\\n\\nhttps://docs.github.com/en/actions/using-jobs/using-a-matrix-for-your-jobs#defining-the-maximum-number-of-concurrent-jobs --\x3e\\n\\n## Handling glitches in the matrix\\n\\n### Matrix job failures\\nIt\'s true that jobs run in parallel and are independent of each other. But when they are spin up by a matrix, by default, all are cancelled if one of them fails. This is not always desirable, so you can change that behaviour.\\n\\n| Option | Scope | Effects |\\n| --- | --- | --- |\\n| `fail-fast: <boolean>` | Entire matrix | Default: `true`. Determines if **all** ongoing and queued matrix jobs will be cancelled if one job fails |\\n| `continue-on-error: <boolean>` | Job level | Determines if a failure on a job will bypass `fail-fast: true` |\\n| `matrix.experimental: <boolean>` | Job level | Allows jobs to have different `continue-on-error` values |\\n\\n```yaml title=\\".github/workflows/testing.yaml\\" showLineNumbers\\njobs:\\n  unit-tests:\\n    continue-on-error: false  # default value, can be ommited\\n    strategy:\\n      // highlight-start\\n      fail-fast: true           # default value, can be ommited\\n      // highlight-end\\n      matrix:\\n        node-version: [14, 16, 18]\\n      runs-on: ubuntu-latest\\n    steps:\\n      (...)\\n```\\n\\n#### `fail-fast`\\n\\n`fail-fast` is  pretty straightforward. `true` will short-circuit the matrix if one job fails; `false` makes all jobs run independently of each other regardless of job failures.\\n\\n#### `continue-on-error`\\n\\n`continue-on-error` is a way to add an exclusion to the `fail-fast` setting.\\n\\nA matrix with `fail-fast: true` will not fail if `continue-on-error` is `true`. The opposite also stands: a matrix with `fail-fast: false` will fail if `continue-on-error: false` is set.  \\n\\nReminder: `continue-on-error` can be used at the step level to prevent a job from failing should that step fail. It can also be used independently of matrices.\\n\\nIf used alone, `continue-on-error` might at best cancel out `fail-fast`, which is not a big advantage. The real leverage comes when `matrix.experimental is added to the mix.  \\n\\n#### `matrix.experimental`\\n\\n`matrix-experimental` allows matrix jobs to have different `continue-on-error` values. This way, you can granularly define which failing jobs should or shouldn\'t halt ongoing & queued jobs. \\n\\nAn example: you are evaluating a new unstable Node.js version and you want to observe it for a while before release. You don\'t want the whole matrix to fail if your little experiment fails the tests; it should only fail if stable versions raise issues.  \\nTo exclude that one job, you can `include` the new version to the matrix and setting that job as the only one with an experimental value of `true`:\\n\\n<details>\\n<summary>\\n  Show code\\n</summary>\\n<div>\\n\\n```yaml title=\\".github/workflows/testing.yaml\\" showLineNumbers\\njobs:\\n  unit-tests:\\n    strategy:\\n      runs-on: ubuntu-latest\\n      // highlight-start\\n      # ommited fail-fast as it\'s true by default\\n      continue-on-error: ${{ matrix.experimental }} # dynamically set for each job\\n      matrix:\\n        node-version: [18, 20]\\n        experimental: [false] # will populate continue-on-error with false\\n        include:\\n          - node-version: 21\\n            experimental: true # will populate continue-on-error with true, but only for the job running Node v21.\\n      // highlight-end\\n    steps:\\n      (...)\\n```\\n\\nSo the resulting jobs from this matrix will be:\\n\\n- node 18, continue-on-error: false\\n- node 20, continue-on-error: false\\n- node 21, continue-on-error: true\\n\\n</div>\\n</details>\\n\\n### Dynamically-generated matrices: Handling Empty Matrix Error\\n\\n_(This section will make more sense once I add the section about dynamically-generated matrices)_  \\nIf you are using dynamically generated matrices, your workflow may fail if the matrix ends up spinning an empty result:\\n\\n```\\nError when evaluating \'strategy\' for job \'xxxxx\'. .github/workflows/my_worflow.yaml (Line: X, Col: Y): Matrix must define at least one vector\\n\\n```\\n\\nThe fix is to execute the dependent job with a conditional:\\n\\n```yaml\\napply:\\n  name: \'${{ matrix.account }}\'\\n  needs: generate\\n  // highlight-next-line\\n  if: ${{ fromJson(needs.generate.outputs.matrix).include[0] }}\\n  strategy:\\n    fail-fast: false\\n    matrix:\\n      include: ${{ fromJSON(needs.generate.outputs.matrix) }}\\n```\\nWhat is happening here?  \\n\\nThe conditional converts the JSON string to an object, and checks if the `include` key has at least one map. Use fearlessly, because matrices contain the `includes` property whenever a  matrix has any elements - it doesn\'t depend on using he `include` keyword explicitly on the matrix definition.  \\n\\n\\n\x3c!-- ## TODO: Advanced use of matrices with reusable workflows\\n\\nI owe this one to [Alexander Kondratskiy in Slack Overflow](https://stackoverflow.com/users/436025/alexander-kondratskiy), and it\'s a very cool use case.\\n\\nImagine you have job_B, dependendt on job_A, and both use matrices. job_B will only start running once **all jobs** in matrix A have completed.  \\n\\nBut what if you\'d like job_B_node_18 to be triggered as soon as job_A_node_18 is completed, instead of waiting for who knows how many jobs complete?\\n\\nHis solution was to create a reusable workflow that traversed the matrix and made \\"the entire workflow of jobs run unimpeded, in parallel\\". [Go check that thing of beauty at SO](https://stackoverflow.com/questions/75318609/matrix-strategy-over-entire-workflow-in-github-actions).\\n\\nI\'ll write a post about reusable workflows soon, this post is already ridiculously long \ud83d\ude05 --\x3e\\n\\n\x3c!-- <https://stackoverflow.com/questions/75318609/matrix-strategy-over-entire-workflow-in-github-actions> --\x3e\\n\\n\x3c!-- How to share matrix between jobs https://github.com/orgs/community/discussions/26284\\n\\nHow use strategy/matrix with script https://stackoverflow.com/questions/59977364/github-actions-how-use-strategy-matrix-with-script\\n\\nhttps://michaelheap.com/dynamic-matrix-generation-github-actions/ --\x3e\\n\\n##\xa0WIP Sections\\n\\nCome back later! I\'ve started drafting content for the following sections:\\n\\n- Limitations (e.g. Matrix outputs don\'t work neatly, they require artifacts)\\n- Dynamically-generated Matrices\\n- Resource Management: Max Number of Concurrent Jobs\\n- Advanced use of matrices with reusable workflows\\n- Sharing a matrix between jobs"},{"id":"github-actions-data-flow","metadata":{"permalink":"/blog/tech/github-actions-data-flow","source":"@site/tech/2023-10-31-github-actions-data-flow/index.md","title":"GitHub Actions: Data Flow & Data Persistence","description":"In Github Actions, by default, data is not inherently persistent or available to the whole pipeline. Every step has is its own process, every job has its own runner. By default, whatever data emerges in a job, ends with it.","date":"2023-10-31T00:00:00.000Z","tags":[{"label":"github actions","permalink":"/blog/tech/tags/github-actions"},{"label":"ci/cd","permalink":"/blog/tech/tags/ci-cd"},{"label":"pipeline","permalink":"/blog/tech/tags/pipeline"},{"label":"env","permalink":"/blog/tech/tags/env"},{"label":"outputs","permalink":"/blog/tech/tags/outputs"},{"label":"artefacts","permalink":"/blog/tech/tags/artefacts"},{"label":"cache","permalink":"/blog/tech/tags/cache"}],"readingTime":9.54,"hasTruncateMarker":true,"authors":[{"name":"Manu Magalh\xe3es","title":"DevSecOps Engineer","url":"https://github.com/magmanu","imageURL":"https://github.com/magmanu.png","key":"manu"}],"frontMatter":{"slug":"github-actions-data-flow","title":"GitHub Actions: Data Flow & Data Persistence","authors":"manu","tags":["github actions","ci/cd","pipeline","env","outputs","artefacts","cache"]},"unlisted":false,"prevItem":{"title":"An Ultimate Guide to Matrices in GitHub Actions, from Basic to Advanced","permalink":"/blog/tech/matrices-github-actions"},"nextItem":{"title":"Generating Dynamic JSON in Terraform","permalink":"/blog/tech/dynamic-json-in-terraform"}},"content":"In Github Actions, by default, data is not inherently persistent or available to the whole pipeline. Every step has is its own process, every job has its own runner. By default, whatever data emerges in a job, ends with it.\\n\\nHow do we pass data from one process to the other, or save it for the next process?\\n\\nA short sweet answer:\\n\\n| Strategy | Data | Scope | Persistence | Explanation | Example |\\n| --- | --- | --- | --- | --- | --- |\\n| `env` | Values | Job (internal) | Ephemeral | Propagates _data_ <br/> between _steps_ <br/>  in the same _job_ | Pass a boolean to control whether the next step should run |\\n| `outputs` | Values | Workflow (internal) | Ephemeral | Propagates _data_ <br/> between _jobs/steps_ <br/>  in the same _workflow_ | Pass a deployment id to the next job |\\n| artefacts | Files | Workflow (internal & external) | Persistent | Propagates _files_ <br/> between _jobs/workflows_ | Pass the project build to different test jobs running in parallel  <br/><br/> _Intended for frequently changing data. Files are available for download after the workflow finishes._ |\\n| cache | Files | Workflow (internal & external) | Persistent | Propagates _files_ <br/> inside and between _workflows_ <br/>  in the same _repository_  | Cache npm packages for use in different workflow runs. <br/><br/> _Intended for files that don\'t change much._ |\\n\\nFor a completer answer: read on.  \\nAll the workflow examples in this article [can be found as files here](https://github.com/magmanu/blog/tree/main/demos/github-actions-data-flow), along with a copy of the respective redacted logs.\\n\\n\x3c!--truncate--\x3e\\n\\n## Using `env`\\n\\nIt\'s pretty simple to create a data flow between steps: define a key-value pair and write it to the `GITHUB_ENV` environment file, using the appropriate syntax for your shell. See examples below in bash and python:\\n\\n<details>\\n   <summary>\\n     Show code\\n   </summary>\\n   <div>\\n\\n```yaml title=\\"/.github/workflows/using_env.yaml\\"\\n    steps:\\n      - name: Two ways to set environment variable with sh\\n      # Warning: in this step, the input is not sanitized or validated\\n        shell: bash\\n        run: |\\n          # No print to the logs.\\n          random_wiki_article_1=$(curl -L -X GET \\"https://en.wikipedia.org/api/rest_v1/page/random/summary\\" | jq .title)\\n          echo \\"$random_wiki_article_1\\"\\n          echo \\"ARTICLE_1=$random_wiki_article_1\\" >> \\"$GITHUB_ENV\\"\\n          # \ud83d\udc09 Print the variable in the logs: only for non-senstive data!\\n          random_wiki_article_2=$(curl -L -X GET \\"https://en.wikipedia.org/api/rest_v1/page/random/summary\\" | jq .title)\\n          echo \\"ARTICLE_2=$random_wiki_article_2\\" | tee -a \\"$GITHUB_ENV\\"\\n\\n      - name: Set environment variable with python\\n        shell: python\\n        # if using \\"write\\", use \\\\n when creating multiple vars\\n        # with \\"print\\", you can omit \\\\n\\n        run: |\\n          from os import environ as env\\n          with open(env.get(\'GITHUB_ENV\', None), \'a\') as ghenv:\\n            ghenv.write(\\"SUBJECT=Sun\\\\n\\")\\n            print(\\"STATE=radiant\\", file=ghenv)\\n            print(\\"TIME=today\\", file=ghenv)\\n          \\n      - name: \ud83d\udee1\ufe0f Retrieving values securely\\n        # observe that ARTICLE_1 was not sanitized or validated, so it\'s vulnerable to injection attacks.\\n        # The approach below prevents the issue by setting env.ARTICLE_1 as an argument to the script.\\n        # It also gives you the chance to rename the variables\\n        env:\\n          WHO: ${{ env.SUBJECT }}\\n          WHAT: ${{ env.ARTICLE_1 }}\\n          WHEN: ${{ env.TIME }}\\n        run: |\\n          echo \\"$WHO read about $WHAT $WHEN.\\"\\n        \\n      - name: \ud83d\udc09 Retrieving values in a potentially vulnerable way\\n        # This approach is vulnerable to injection attacks!\\n        # Only use it if you have control over the input\\n        shell: bash\\n        run: |\\n          echo \\"${{ env.SUBJECT }} is ${{ env.STATE }} ${{ env.TIME }}.\\"\\n```\\n\\n   </div>\\n</details>\\n\\n#### Debugging tip\\nTo list all the environment variables available in a job, add this tiny step:\\n\\n```- run: env```\\n\\n## Using `outputs`\\n\\nOutputs are available to all steps in the same job, and to any subsequent job that `needs` it.  \\nThe output is always an unicode **string**.  \\n\\nAnd obviously, jobs that depend on an `output` will not run in parallel with the job that produces the output.\\n\\n<details>\\n   <summary>\\n     Show code\\n   </summary>\\n   <div>\\n\\nFor simplicity, I show how to set the output in bash, but you can use any shell of your choice.  \\n\\n```yaml title=\\"/.github/workflows/outputs-for-different-job.yaml\\"\\njobs:\\n  setting-outputs:\\n    runs-on: ubuntu-latest\\n    // highlight-start\\n    outputs:  # Required: name the output in the job level so it\'s available to other jobs\\n      person_name: ${{ steps.use-hardcoded-value.outputs.NAME }}\\n      location: ${{ steps.use-dynamic-value.outputs.LOCATION }}\\n    // highlight-end\\n    steps:\\n      - id: use-hardcoded-value\\n        run: |\\n          // highlight-next-line\\n          echo \\"NAME=Marcela\\" >> \\"$GITHUB_OUTPUT\\"\\n      \\n      - id: use-dynamic-value\\n        # note the use of jq -c to get the value as a single line\\n        run: |\\n          location=$(curl -H \\"Accept: application/json\\" https://randomuser.me/api/ | jq -c .results[].location)\\n          // highlight-next-line\\n          echo \\"LOCATION=$location\\" > \\"$GITHUB_OUTPUT\\"\\n\\n  retrieving-outputs:\\n    runs-on: ubuntu-latest\\n    needs: setting-outputs\\n    steps:\\n      - name: Greet to location\\n        run: |\\n          COUNTRY=$(echo $GEODATA | jq -r . | jq .country)\\n          echo \\"Hello $NAME, welcome to $COUNTRY!\\"\\n         // highlight-start\\n        env:\\n          NAME: ${{needs.setting-outputs.outputs.person_name}}\\n          GEODATA: ${{ needs.setting-outputs.outputs.location }}\\n        // highlight-end\\n```\\n   </div>\\n</details>\\n\\nEven though it\'s recommended to use `env` to pass data between steps, `outputs` can be used for that purpose as well. This is useful when a value is required both in the current job and in subsequent jobs.  \\n\\n<details>\\n   <summary>\\n     Show code\\n   </summary>\\n   <div>\\n\\nThe previous example showed how to use outputs in different jobs.  \\nTo use an output the same job, simply add the code in the highlighted section.\\n\\n```yaml title=\\"/.github/workflows/outputs-for-same-job.yaml\\"\\njobs:\\n  extract:\\n    runs-on: ubuntu-latest\\n    outputs:\\n      person_name: ${{ steps.generate-hardcoded-value.outputs.name }}\\n      location: ${{ steps.enerate-dynamic-value.outputs.location }}\\n        steps:\\n      - id: generate-hardcoded-value\\n        run: |\\n          echo \\"NAME=Marcela\\" >> \\"$GITHUB_OUTPUT\\"\\n      - id: generate-dynamic-value\\n        run: |\\n          location=$(curl -H \\"Accept: application/json\\" https://randomuser.me/api/ | jq .results[].location | jq @json) \\n          echo \\"LOCATION=$location\\" >> \\"$GITHUB_OUTPUT\\"\\n      // highlight-start\\n      - name: Consume output in same job\\n        run: |\\n          echo \\"$PERSON, you\'re in $GEODATA, so we\'ve updated your timezone to GMT$OFFSET.\\"\\n        env:\\n          PERSON: ${{ steps.use-hardcoded-value.outputs.NAME }}\\n          # use fromJSON() when filtering the output value at the env level\\n          # See more about object filtering in \\n          # https://docs.github.com/en/actions/learn-github-actions/expressions#object-filters\\n          GEODATA: ${{ fromJSON(steps.use-dynamic-value.outputs.LOCATION).country }}\\n          OFFSET: ${{ fromJSON(steps.use-dynamic-value.outputs.LOCATION).timezone.offset }}\\n      // highlight-end\\n\\n    (...)\\n```\\n   </div>\\n</details>\\n\\n:::info Helpful debugging info\\n\\n- An individual output should be 1MB max. \\n- All outputs combined should not exceed 50MB.\\n\\n:::\\n\\n<br/>\\n\\n:::tip Real life XP\\n\\n`GITHUB_OUTPUT` expects a one-line string.  \\nIf you need a multiline output, assign it to a variable and write to the output as follows:\\n\\n```bash\\necho \\"PAYLOAD_NAME<<EOF\\"$\'\\\\n\'\\"$payload_var\\"$\'\\\\n\'EOF >> \\"$GITHUB_OUTPUT\\".\\n```\\n\\n:::\\n\\n## Using artefacts\\n\\nFrom the docs: \\"_Use artefacts when you want to save files produced by a job to view after a workflow run has ended, such as built binaries or build logs_.\\"\\n\\n### Uploading artefacts\\n\\nYou can:\\n- select one or multiple files to be bundled as an artifact.  \\n- use wildcards, multiple paths and exclusion patterns in the usual GitHub Actions syntax.  \\n- set a retention period for the artefact.\\n\\n```yaml title=\\"/.github/workflows/handle-artefacts.yaml\\"\\njobs:\\n  upload:\\n    runs-on: ubuntu-latest\\n    steps:\\n      - name: Checkout\\n        uses: actions/checkout@v4\\n      - name: Upload log files\\n        uses: actions/upload-artifact@v4\\n        with:\\n          name: all-logs      # artefact name\\n          path: |             # path to files to be included in the artifact.\\n            **/log*.txt       # relative paths are rooted against $GITHUB_WORKSPACE\\n          retention-days: 1\\n          if-no-files-found: error # force step to fail if the content for the artefact is not found\\n```\\n\\nNote that maximum retention period [can be defined at repo, organisation, or enterprise level](https://docs.github.com/en/actions/learn-github-actions/usage-limits-billing-and-administration#artifact-and-log-retention-policy). There\'s a max of 90 days for public repos and 400 days for private repos. If you lower the retention period, you\'ll have more non-billed space ;) \\n\\n### Downloading artefacts\\n\\nTo retrieve the artefact, you can use:\\n- the [Github UI](https://docs.github.com/en/actions/managing-workflow-runs/downloading-workflow-artifacts)\\n- the [Github API](https://docs.github.com/en/rest/actions/artifacts?apiVersion=2022-11-28#download-an-artifact)\\n- the [`gh` cli](https://docs.github.com/en/actions/managing-workflow-runs/downloading-workflow-artifacts?tool=cli)\\n- the official [`actions/download-artifact`](https://github.com/marketplace/actions/download-a-build-artifact) action, if you need to retrieve artifacts programmatically. From `v4`, the action allows you to download artefacts from a different workflows or repos, as long as you provide a token. (\ud83d\udee1\ufe0f: it\'s recommended to use a GitHub App rather than a PAT for professional projects.)\\n\\nLet\'s see how to retrieve the artefact we created in the previous example using `actions/download-artifact`:\\n\\n```yaml title=\\"/.github/workflows/handle-artefacts.yaml\\"\\ndownload:\\n    runs-on: ubuntu-latest\\n    needs: upload\\n    steps:\\n      - name: Download log files\\n        id: download-artifacts\\n        uses: actions/download-artifact@v4\\n        with:\\n          name: all-logs  # note it uses the name defined in the upload step\\n        \\n      - name: Pass artifact path to python\\n        shell: python\\n        run: |\\n          import os\\n          from glob import glob\\n          artifact_path = os.environ.get(\\"ARTIFACT_PATH\\", \\"\\")\\n          glob_list = glob(artifact_path + \\"/*.txt\\")\\n          for filename in glob_list:\\n              with open(filename, \\"r\\", encoding=\\"UTF-8\\") as f:\\n                  content = f.read()\\n                  print(content)\\n        env:\\n          ARTIFACT_PATH: ${{ steps.download-artifacts.outputs.download-path }}\\n```\\n\\nAll the zipping and unzipping of the artifacts is automatically handled by the actions.\\n\\n### Deleting artefacts\\n\\nTo delete an artefact, you can:\\n- use the [Github UI](https://docs.github.com/en/actions/managing-workflow-runs/removing-workflow-artifacts)\\n- use the [Github API](https://docs.github.com/en/rest/reference/actions#delete-an-artifact)\\n- write a [custom script using the Github API](https://gist.github.com/qwe321qwe321qwe321/efae4569576006624c34f23b2dd76a58) or using a [community action](https://github.com/GeekyEggo/delete-artifact).\\n\\n## Using cache\\n\\n:::danger \ud83d\udc09 Security warning\\n\\nDo not store sensitive information in the cache (beware of configuration files containing secrets), as the cache is accessible to anyone who can create a PR on the repository, even on forks.\\n\\n:::\\n\\nWhen we\'re handling data that is pretty stable and repeatedly used (like dependencies), we can do better than re-generating them every time: we can cache them for better performance.\\n\\nIn the example below, we\'re caching the `pip` dependencies for a Python project. Note that we have added the cache step before the pip install step. The idea is that the install will only happen if the cache is not good or available: \\n\\n\\n```yaml title=\\"/.github/workflows/cache.yaml\\"\\njobs:\\n  cache:\\n    runs-on: ubuntu-latest\\n    steps:\\n      - uses: actions/checkout@v4\\n      - uses: actions/setup-python@v5\\n        with:\\n          python-version: \'3.12\'\\n          cache: \'pip\'\\n          cache-dependency-path: |\\n            **/requirements.txt\\n      \\n      - name: Get pip cache dir\\n        id: pip-cache\\n        run: |\\n          echo \\"dir=$(pip cache dir)\\" >> $GITHUB_OUTPUT\\n\\n      - name: Handle cache for Python dependencies\\n        uses: actions/cache@v3\\n        The cache action requires a `path` to the cache and a `key`. The `key` is used to retrieve the cache and to recreate it next time. \\n        id: cache\\n        with:\\n          # path: location of files to cache\\n          path: ${{ steps.pip-cache.outputs.dir }} \\n          # key: unique id used to retrieve and recreate the cache\\n          key: ${{ runner.os }}-pip-${{ hashFiles(\'**/requirements.txt\') }} \\n        \\n      - name: Install dependencies if not found in cache\\n        if: steps.cache.outputs.cache-hit != \'true\'\\n        run: pip install -r requirements.txt\\n```\\n\\n### Setting the cache\\n\\nThe first time the workflow runs, the cache is obviously empty. Therefore, the output `cache-hit` (native to the official `actions/cache` action), will return `false`, which in turn makes our workflow run the install step.  \\n[*Check logs*](https://raw.githubusercontent.com/magmanu/blog/main/demos/github-actions-data-flow/31_data_flow_cache_set-cache.txt)\\n\\nHowever, a small magic happens too: a post-cache step, automatically added by `action/cache` at the end of the job, will look at the keys your provided and add the files to the cache.\\n\\n### Retrieving the cache\\n\\nAs long as nothing has changed in your dependency manifest, the next time `actions/cache` runs for that path and key, the action will find a `cache-hit`and the workflow will safely skip the install step.  \\n[*Check logs*](https://raw.githubusercontent.com/magmanu/blog/main/demos/github-actions-data-flow/32_data_flow_cache_use-cache.txt)\\n\\n\\n### Updating the cache\\n\\nHave you noted the `hashFiles` function used in the `key` argument?  \\nThis is a function provided by GitHub Actions that creates a unique hash value based on a file path. When the hash value doesn\'t match, it means that there was a change in the file - in our case, the dependency manifest.  \\n\\nIf the dependencies changed (even a single patch), the cache is no good anymore and the `cache-hit` output will allow `pip install` to run. And then we\'re back to square one: dependencies are installed and the cache is updated on a post-cache job.  \\n[*Check the logs*](https://raw.githubusercontent.com/magmanu/blog/main/demos/github-actions-data-flow/33_data_flow_cache_update-cache.txt)\\n\\n### Last notes about caching\\n\\n- If you\u2019re using self-hosted runners, the option to self-store the cache is only available in Enterprise plans.\\n- This action `actions/cache` manages the cache centrally. This means that the cache is available to (and updatable by) all jobs in the same repository - and even to other workflows.\\n- Read more about [caching strategies here](https://github.com/actions/cache/blob/main/caching-strategies.md).\\n\\n\\nThat was a long post, phew.  \\nSee you later!  :)"},{"id":"dynamic-json-in-terraform","metadata":{"permalink":"/blog/tech/dynamic-json-in-terraform","source":"@site/tech/2023-03-18-dynamic-json-in-terraform/index.md","title":"Generating Dynamic JSON in Terraform","description":"In this article I\u2019ll use Step Functions as a case study, but you can do it with whatever resources you want.","date":"2023-03-18T00:00:00.000Z","tags":[{"label":"infra","permalink":"/blog/tech/tags/infra"},{"label":"terraform","permalink":"/blog/tech/tags/terraform"},{"label":"IaC","permalink":"/blog/tech/tags/ia-c"},{"label":"step functions","permalink":"/blog/tech/tags/step-functions"},{"label":"devops","permalink":"/blog/tech/tags/devops"}],"readingTime":8.72,"hasTruncateMarker":true,"authors":[{"name":"Manu Magalh\xe3es","title":"DevSecOps Engineer","url":"https://github.com/magmanu","imageURL":"https://github.com/magmanu.png","key":"manu"}],"frontMatter":{"slug":"dynamic-json-in-terraform","title":"Generating Dynamic JSON in Terraform","authors":"manu","tags":["infra","terraform","IaC","step functions","devops"]},"unlisted":false,"prevItem":{"title":"GitHub Actions: Data Flow & Data Persistence","permalink":"/blog/tech/github-actions-data-flow"},"nextItem":{"title":"Bypassing Terraform error: \u201cThe true and false result expressions must have consistent types\u201d","permalink":"/blog/tech/terraform-ternary-error"}},"content":"In this article I\u2019ll use Step Functions as a case study, but you can do it with whatever resources you want.\\n\\n## What we\u2019re going to do\\nThe following JSON will become dynamic. We\u2019ll replace static values with:\\n1. [A variable](#case-1-injecting-a-variable-into-json)\\n2. [A dynamic list](#case-2-injecting-a-list-into-a-json)\\n3. [One or more dynamic objects](#case-3-injecting-an-object-into-a-json)\\n\\n\x3c!--truncate--\x3e\\n\\n(I\u2019ve added non idiomatic comments inside the code blocks, but it\u2019s just to show what exactly we\u2019re doing.)\\n\\n```json title=\'Reference json\'\\n{\\n    \\"Comment\\": \\"My state machine\\",\\n    \\"StartAt\\": \\"Choice\\",\\n    \\"States\\": {\\n        \\"Handle Notification\\": {\\n            \\"Type\\": \\"Task\\",\\n            \\"Resource\\": \\"arn:aws:states:::lambda:invoke\\",\\n            \\"OutputPath\\": \\"$.Payload\\",\\n            \\"Parameters\\": {\\n              \\"Payload.$\\": \\"$\\",\\n              // highlight-start\\n              -----\x3e Case 1: Replace with dynamic string\\n              \\"FunctionName\\": \\"my_function_name\\"\\n              // highlight-end\\n            },\\n            \\"End\\": true\\n        },\\n        \\"Choice\\": {\\n          \\"Type\\": \\"Choice\\",\\n          // highlight-start\\n          ----\x3e Case 2: Replace with dynamic list\\n          \\"Choices\\": [\\n             {                 \\n               \\"IsPresent\\": true,                      \\n               \\"Next\\": \\"SSM Execution-InstanceId\\",     \\n               \\"Variable\\": \\"$.InstanceId\\"              \\n              },               \\n              {                \\n               \\"IsPresent\\": true,                      \\n               \\"Next\\": \\"SSM Execution-SecurityGroupIds\\", \\n               \\"Variable\\": \\"$.SecurityGroupIds\\"        \\n              }                \\n            ],\\n            // highlight-end\\n            \\"Default\\": \\"Pass\\"\\n        },\\n        // highlight-start\\n        ----\x3e Case 3: Replace with one or more dynamic objects\\n        \\"SSM Execution-InstanceId\\": {\\n            \\"Next\\": \\"Pass\\",\\n            \\"Parameters\\": {\\n                \\"DocumentName.$\\": \\"$.DocumentName\\",\\n                \\"Parameters\\": {\\n                    \\"InstanceId.$\\": \\"States.Array($.InstanceId)\\"\\n                }\\n              },\\n            \\"Resource\\": \\"arn:aws:states:::aws-sdk:ssm:startAutomationExecution\\",\\n            \\"ResultPath\\": \\"$.TaskResult\\",\\n            \\"Type\\": \\"Task\\" \\n        },\\n        \\"SSM Execution-SecurityGroupIds\\": {\\n            \\"Next\\": \\"Pass\\",\\n            \\"Parameters\\": {\\n                \\"DocumentName.$\\": \\"$.DocumentName\\",\\n                \\"Parameters\\": {\\n                    \\"SecurityGroupIds.$\\": \\"States.Array($.SecurityGroupIds)\\"\\n                }\\n            },\\n            \\"Resource\\": \\"arn:aws:states:::aws-sdk:ssm:startAutomationExecution\\",\\n            \\"ResultPath\\": \\"$.TaskResult\\",          \\n            \\"Type\\": \\"Task\\" \\n        },\\n        // highlight-end\\n        \\"Pass\\": {\\n            \\"Type\\": \\"Pass\\",\\n            \\"End\\": true\\n        }\\n    }\\n}\\n```\\n\\nFirst, save the JSON above in a template format. You can use the extension `.tpl` or `.tftpl`. Although Terraform recommends using the second option, there\'s no hard rule about which to use.\\n\\n## Case 1: Injecting a variable into JSON\\n\\nLet\u2019s begin by updating the `FunctionName` value in the template, using string interpolation:\\n\\n```json title=\'modules/templates/stepfunction_definition.tftpl (extract)\'\\n\\n{\\"Handle Notification\\": {\\n        \\"Type\\": \\"Task\\",\\n        \\"Resource\\": \\"arn:aws:states:::lambda:invoke\\",\\n        \\"OutputPath\\": \\"$.Payload\\",\\n        \\"Parameters\\": {\\n          \\"Payload.$\\": \\"$\\",\\n          // highlight-next-line\\n          \\"FunctionName\\": \\"${lambda_function_name}\\"\\n        },\\n        \\"End\\": true\\n      }\\n}\\n```\\n\\nTo render the template as a valid JSON, we can use Terraform\u2019s `templatefile` function. It takes two arguments: the template file path and a map of variables that will be injected at runtime. For the sake of the argument, let\u2019s keep our new module as dynamic as possible. Locals, variables and resources are also included just to show how things work together.\\n\\n```hcl title=\'modules/stepfunction.tf (full file)\'\\n\\nlocals {\\n    file = templatefile(\\"${path.module}/templates/stepfunction_definition.tpl\\", {\\n        lambda_function_name = var.lambda_function_name\\n    })\\n}\\nvariable \\"lambda_function_name\\" {\\n    description = \\"Lambda function name\\"\\n    type        = string\\n}\\n\\nresource \\"aws_sfn_state_machine\\" \\"sfn_state_machine\\" {\\n    name        = var.step_function_name\\n    role_arn    = var.sf_role_arn\\n    definition  = local.file\\n}\\n```\\n\\nNow we write a top level `main.tf`, where the actual values go.\\n\\n```hcl title=\'main.tf (full file)\'\\n\\nmodule \\"my_step_function\\" {\\n  source                = \\"./modules/step_functions\\"\\n  step_function_name    = \\"autoremediation_sf\\"\\n  sf_role_arn           = \\"arn:aws:iam::123456789012:role/autoremediation_sf\\"\\n  lambda_function_name  = \\"autoremediation_lambda\\"\\n}\\n```\\n\\nTo check that your variable was successfully injected, run `terraform init && terraform plan`.\\n\\n## Case 2: Injecting a list into a JSON\\n\\nAs a refresher, we will abstract this whole list:\\n\\n```json title=\'modules/templates/stepfunction_definition.tftpl (extract)\'\\n\\n\\"Choice\\": {\\n    \\"Type\\": \\"Choice\\",\\n    // highlight-start\\n    \\"Choices\\": [                             \\n        {                 \\n          \\"IsPresent\\": true,                      \\n          \\"Next\\": \\"SSM Execution-InstanceId\\",     \\n          \\"Variable\\": \\"$.InstanceId\\"              \\n        },                \\n        {\\n          \\"IsPresent\\": true,                      \\n          \\"Next\\": \\"SSM Execution-SecurityGroupIds\\", |\\n          \\"Variable\\": \\"$.SecurityGroupIds\\"        \\n        }                 \\n    ],                                       \\n    // highlight-end\\n    \\"Default\\": \\"Pass\\"\\n}\\n```\\n\\nSo let\u2019s refactor the template again. We\u2019ll use a Terraform function called `jsonencode`, as it ensures that the list of objects we\u2019re passing to the template will be properly rendered as JSON:\\n\\n```json title=\'modules/templates/stepfunction_definition.tftpl (extract)\'\\n\\n\\"Choice\\": {\\n    \\"Type\\": \\"Choice\\",\\n    // highlight-next-line\\n    \\"Choices\\": ${jsonencode(choices_list)},\\n    \\"Default\\": \\"Pass\\"\\n    }\\n ```\\n\\nNow, we\u2019ll try two ways to feed the template with our dynamic list.\\n\\n### The simplest solution: static list\\n\\nOne option is to create a variable that will pass a ready-made list for you:\\n\\n```hcl title=\'modules/stepfunction.tf (extract)\'\\n\\nlocals {\\n    file = templatefile(\\"${path.module}/templates/stepfunction_definition.tpl\\", {\\n        lambda_function_name = var.lambda_function_name\\n        // highlight-next-line\\n        choices_list = var.my_list\\n    })\\n}\\n// highlight-start\\nvariable \\"my_list\\" {\\n    description = \\"A list of whatever\\"\\n    type = list\\n// highlight-end\\n}\\n```\\n\\nThen, in top level module, pass the variable values. The JSON is now dynamic, but the list is pretty much \u201cstatic\u201d.\\n\\n```hcl title=\'main.tf (full file)\'\\n\\nmodule \\"my_step_function\\" {\\n    source                = \\"./modules/step_functions\\"\\n    step_function_name    = \\"autoremediation_sf\\"\\n    sf_role_arn           = \\"arn:aws:iam::123456789012:role/autoremediation_sf\\"\\n    lambda_function_name  = \\"autoremediation_lambda\\"\\n\\n    // highlight-start\\n    my_list = [{\\n        \\"IsPresent\\": true,\\n        \\"Next\\": \\"SSM Execution-InstanceId\\",\\n        \\"Variable\\": \\"$.InstanceId\\"\\n    }]\\n    // highlight-end\\n}\\n```\\n\\n### The logic-heavy solution: dynamic list\\n\\nWhat if you want the list itself to be dynamic too? For example, you want the values for `Next` and `Variable` to be extracted from a parameter and injected to list before it\u2019s sent to the json?\\nIn our example, the values that populate `Next` and `Variable` come from keys provided by `ssm_params`, in the top level file:\\n\\n```hcl title=\'main.tf (extract)\'\\n\\nmodule \\"my_step_function\\" {\\n    source                = \\"./modules/step_functions\\"\\n    step_function_name    = \\"autoremediation_sf\\"\\n    sf_role_arn           = \\"arn:aws:iam::123456789012:role/autoremediation_sf\\"\\n    lambda_function_name  = \\"autoremediation_lambda\\"\\n\\n    // highlight-start\\n    #  Static list is no longer here\\n    ssm_params = [\\n        {\\"InstanceId\\": \\"States.Array($.InstanceId)\\"},    \\n        {\\"SecurityGroupIds\\": \\"States.Array($.SecurityGroupIds)\\"}\\n    ]\\n    // highlight-end\\n}\\n```\\n\\nNow our module becomes:\\n\\n```hcl title=\'modules/stepfunction.tf (full file)\'\\n\\nlocals {\\n    choices_list = flatten([for item in var.ssm_params: [\\n        for key, value in item : {\\n            \\"IsPresent\\": true,\\n            \\"Next\\": \\"SSM Execution-${key}\\"\\n            \\"Variable\\": \\"$.${key}\\",\\n        }]\\n    ])\\n    file = templatefile(\\"${path.module}/templates/stepfunction_definition.tpl\\", {\\n        lambda_function_name = var.lambda_function_name,\\n        choices_list = local.choices_list\\n    })\\n}\\nvariable \\"lambda_function_name\\" {\\n    description = \\"Lambda function name\\"\\n    type        = string\\n}\\nvariable \\"ssm_params\\" {\\n    description = \\"List of SSM param objects to be injected into the Step Function definition\\"\\n    type = list\\n}\\nresource \\"aws_sfn_state_machine\\" \\"sfn_state_machine\\" {\\n    name        = var.step_function_name\\n    role_arn    = aws_iam_role.step_function_role_arn\\n    definition  = local.file\\n}\\n```\\n\\nStarting with the line `choices_list = flatten([for item in var.ssm_params:` ignore `flatten` for a bit and look at the loop. Because `ssm_params` is a list, we have to loop through it to access each item. The loop is wrapped in square brackets, which means the output will be a list. The `:` that follows is just part of the loop syntax.\\n\\nNext line, another loop: `for key, value in item : {` . This loop goes through each object in `ssm_params` to access key and the value, so we can extract and restructure the data. The loop is also wrapped with square brackets, meaning that every object will unfortunately be inside their own list.\\n\\nNow, remember `flatten`? We\'re using this Terraform function because, as each loop returns a list, the result looks like this: `[[obj1],[obj2]]`. But we need a clean list of objects, and flatten can get rid of the unnecessary nesting.\\n\\nDone.\\n\\n## Case 3: Injecting an object into a JSON\\n\\nNow, the most exciting part. We will abstract whole objects. (In this particular case, this means whole steps can become dynamic despite Amazon States Language!)\\nYou see all this code below? We\'re refactoring IT ALL away!\\n\\n```json\\n\\"Steps\\": {\\n    // highlight-start\\n    \\"SSM Execution-InstanceId\\": {            \\n            \\"Next\\": \\"Pass\\",\\n            \\"Parameters\\": {\\n                \\"DocumentName.$\\": \\"$.DocumentName\\",\\n                \\"Parameters\\": {                    \\n                    \\"InstanceId.$\\": \\"States.Array($.InstanceId)\\"\\n                }          \\n              },           \\n            \\"Resource\\": \\"arn:aws:states:::aws-sdk:ssm:startAutomationExecution\\",\\n            \\"ResultPath\\": \\"$.TaskResult\\",          \\n            \\"Type\\": \\"Task\\" \\n        },\\n        \\"SSM Execution-SecurityGroupIds\\": {        \\n            \\"Next\\": \\"Pass\\",\\n            \\"Parameters\\": {\\n                \\"DocumentName.$\\": \\"$.DocumentName\\",\\n                \\"Parameters\\": {                    \\n                    \\"SecurityGroupIds.$\\": \\"States.Array($.SecurityGroupIds)\\"\\n                }          \\n            },             \\n            \\"Resource\\": \\"arn:aws:states:::aws-sdk:ssm:startAutomationExecution\\",\\n            \\"ResultPath\\": \\"$.TaskResult\\",          \\n            \\"Type\\": \\"Task\\" \\n        }                                    \\n        // highlight-end\\n}\\n```\\n\\nAs usual, let\u2019s modify the template:\\n\\n```hcl title=\'modules/templates/stepfunction_definition.tftpl (extract)\'\\n\\n\\"States\\": {\\n    \\"Choice\\": {\\n      \\"Type\\": \\"Choice\\",\\n      \\"Choices\\": ${jsonencode(choices)},\\n      \\"Default\\": \\"Pass\\"\\n    },\\n    // highlight-start\\n    %{ for key, data in ssm_execution }\\n        \\"${key}\\": ${jsonencode(data)},\\n    %{ endfor }\\n    \\"Handle Notification\\": {<--- blahblah--\x3e}\\n    }\\n    // highlight-end\\n```\\n\\nHere\u2019s where I ask you to trust Terraform even if your linter freaks out.\\n\\nWhat\u2019s happening? We\u2019re using the [Terraform directive](https://developer.hashicorp.com/terraform/language/expressions/strings#directives) syntax `%{}` to write a dynamic string (yeah, templates are treated as strings).\\n\\nThe line `%{ for key, data in ssm_execution }` informs the `templatefile` function that an iteration is coming, and it will end when it reaches the line `%{ endfor }`. In the meanwhile, it should keep creating a key-value pair with the format `\\"my_key\\": {\\"some\\": \\"json\\"},`.\\n\\nPretty cool, uh?\\n\\nSo now let\u2019s add the local `ssm_execution`, the logic that will populate all this.\\n\\n```hcl title=\'modules/stepfunction.tf (extract)\'\\n\\nlocals {\\n    choices = flatten([for item in var.ssm_params: [\\n        for key, value in item : {\\n            \\"IsPresent\\": true,\\n            \\"Next\\": \\"SSM Execution-${key}\\"\\n            \\"Variable\\": \\"$.${key}\\",\\n        }]\\n    ])\\n\\n    // highlight-start\\n    ssm_execution = merge(flatten([for item in var.ssm_params: [ \\n                    for key, value in item : {\\n                        \\"SSM Execution-${key}\\": {\\n                            \\"Type\\": \\"Task\\",\\n                            \\"Parameters\\": {\\n                              \\"DocumentName.$\\": \\"$.DocumentName\\",\\n                              \\"Parameters\\": {\\"$.${key}\\": \\"${value}\\"}\\n                            },\\n                            \\"Resource\\": \\"arn:aws:states:::aws-sdk:ssm:startAutomationExecution\\",\\n                            \\"Next\\": \\"Handle Notification\\",\\n                            \\"ResultPath\\": \\"$.TaskResult\\"\\n                        }\\n                    }\\n                    ]]\\n                )...)\\n    // highlight-end\\n\\n    file = templatefile(\\"${path.module}/templates/stepfunction_definition.tpl\\", {\\n        lambda_function_name = var.lambda_function_name,\\n        choices_list = local.choices,\\n        // highlight-next-line\\n        ssm_execution = local.ssm_execution\\n    })\\n}\\n```\\n\\nWe\u2019re already familiar with loops and flatten, so let\u2019s talk about the new kids on the block: `merge` and `...`.\\n\\n`merge` is a terraform function that takes multiple objects and merge them together into a single object. So, if we have two objects like\\n\\n```json\\n  {\\n    \\"a\\": 1,\\n    \\"b\\": 2\\n  },\\n  {\\n    \\"c\\": 3,\\n    \\"d\\": 4\\n  }\\n```\\n\\nthe `merge` function will consolidate them to:\\n\\n```json\\n{\\n    \\"a\\": 1,\\n    \\"b\\": 2,\\n    \\"c\\": 3,\\n    \\"d\\": 4\\n}\\n```\\n\\nAnd if you were paying attention, you noticed that the merge function in our module is **not** taking several object as arguments, but a flatten function (which outputs a single list). We have `...` to thank for the trick.\\nIn Terraform, `...` (three dots) work like the Javascript spread operator: it expands our list into separate arguments.\\nSo that\u2019s it. Our final template will literally be half the size of our original JSON, it\u2019s dynamic, reusable, and best of all \u2014 frees you from ever handling Amazon States Language in your project again.\\n\\n## Final files\\n\\n```hcl title=\'modules/templates/stepfunction_definition.tpl\'\\n\\n{\\n    \\"Comment\\": \\"My state machine\\",\\n    \\"StartAt\\": \\"Choice\\",\\n    \\"States\\": {\\n        \\"Handle Notification\\": {\\n            \\"Type\\": \\"Task\\",\\n            \\"Resource\\": \\"arn:aws:states:::lambda:invoke\\",\\n            \\"OutputPath\\": \\"$.Payload\\",\\n            \\"Parameters\\": {\\n              \\"Payload.$\\": \\"$\\",\\n              \\"FunctionName\\": \\"${lambda_function_name}\\"\\n            },\\n            \\"End\\": true\\n        },\\n        \\"Choice\\": {\\n            \\"Type\\": \\"Choice\\",\\n            \\"Choices\\": ${jsonencode(choices)},\\n            \\"Default\\": \\"Pass\\"\\n        },\\n        %{ for key, data in ssm_execution }\\n        \\"${key}\\": ${jsonencode(data)},\\n        %{ endfor }\\n        \\"Pass\\": {\\n            \\"Type\\": \\"Pass\\",\\n            \\"End\\": true\\n        }\\n    }\\n}\\n```\\n\\n```hcl title=\'modules/stepfunction.tf\'\\nlocals {\\n    choices = flatten([for item in var.ssm_params: [\\n        for key, value in item : {\\n            \\"IsPresent\\": true,\\n            \\"Next\\": \\"SSM Execution-${key}\\"\\n            \\"Variable\\": \\"$.${key}\\",\\n        }]\\n    ])\\n\\n    ssm_execution = merge(flatten([for item in var.ssm_params: [ \\n                    for key, value in item : {\\n                        \\"SSM Execution-${key}\\": {\\n                            \\"Type\\": \\"Task\\",\\n                            \\"Parameters\\": {\\n                              \\"DocumentName.$\\": \\"$.DocumentName\\",\\n                              \\"Parameters\\": {\\"$.${key}\\": \\"${value}\\"}\\n                            },\\n                            \\"Resource\\": \\"arn:aws:states:::aws-sdk:ssm:startAutomationExecution\\",\\n                            \\"Next\\": \\"Handle Notification\\",\\n                            \\"ResultPath\\": \\"$.TaskResult\\"\\n                        }\\n                    }\\n                    ]]\\n                )...)\\n\\n    file = templatefile(\\"${path.module}/templates/stepfunction_definition.tpl\\", {\\n        lambda_function_name = var.lambda_function_name,\\n        choices_list = local.choices,\\n        ssm_execution = local.ssm_execution\\n    })\\n}\\n\\nvariable \\"lambda_function_name\\" {\\n    description = \\"Lambda function name\\"\\n    type        = string\\n}\\n\\nvariable \\"ssm_params\\" {\\n    description = \\"List of SSM param objects to be injected into the Step Function definition\\"\\n    type = list\\n}\\n\\nresource \\"aws_sfn_state_machine\\" \\"sfn_state_machine\\" {\\n    name        = var.step_function_name\\n    role_arn    = var.sf_role_arn\\n    definition  = local.file\\n}\\n```\\n\\n```hcl title=\'main.tf\'\\n\\nmodule \\"my_step_function\\" {\\n    source                = \\"./modules/step_functions\\"\\n    step_function_name    = \\"autoremediation_sf\\"\\n    sf_role_arn           = \\"arn:aws:iam::123456789012:role/autoremediation_sf\\"\\n    lambda_function_name  = \\"autoremediation_lambda\\"\\n\\n    ssm_params = [                \\n        {\\"InstanceId\\": \\"States.Array($.InstanceId)\\"},    \\n        {\\"SecurityGroupIds\\": \\"States.Array($.SecurityGroupIds)\\"}\\n    ]\\n}\\n```"},{"id":"terraform-ternary-error","metadata":{"permalink":"/blog/tech/terraform-ternary-error","source":"@site/tech/2023-03-18-terraform-ternary-errors/index.md","title":"Bypassing Terraform error: \u201cThe true and false result expressions must have consistent types\u201d","description":"Have you ever came across this Terraform error \u2014 when you intentionally want your ternary to output different types?","date":"2023-03-18T00:00:00.000Z","tags":[{"label":"infra","permalink":"/blog/tech/tags/infra"},{"label":"terraform","permalink":"/blog/tech/tags/terraform"},{"label":"workarounds","permalink":"/blog/tech/tags/workarounds"},{"label":"devops","permalink":"/blog/tech/tags/devops"}],"readingTime":1.91,"hasTruncateMarker":true,"authors":[{"name":"Manu Magalh\xe3es","title":"DevSecOps Engineer","url":"https://github.com/magmanu","imageURL":"https://github.com/magmanu.png","key":"manu"}],"frontMatter":{"slug":"terraform-ternary-error","title":"Bypassing Terraform error: \u201cThe true and false result expressions must have consistent types\u201d","authors":"manu","tags":["infra","terraform","workarounds","devops"]},"unlisted":false,"prevItem":{"title":"Generating Dynamic JSON in Terraform","permalink":"/blog/tech/dynamic-json-in-terraform"},"nextItem":{"title":"How to Migrate CodeCommit to GitHub \u2014 and Keep your Amplify Pipeline","permalink":"/blog/tech/migrate-codecommit-to-github"}},"content":"Have you ever came across this Terraform error \u2014 when you intentionally want your ternary to output different types?\\n\\n>The true and false result expressions must have consistent types\\n\\nTo bypass this Terraform limitation, check the tip below. I\'ll follow it with two examples to clarify the usage:\\n\\n```hcl\\nattribute = [\\n    <desired output if true>, \\n    <desired output if false>\\n    ][<condition> ? 0 : 1]\\n```\\n\\n\x3c!--truncate--\x3e\\n\\n## Simple example\\n\\n```hcl\\nlocals {\\n    dynamic_value = [\\n      {\\"region\\": \\"${var.region}\\"}, \\n      \\"unavailable\\"\\n      ][var.region == \\"eu-west-1\\" ? 0 : 1 ]\\n}\\n```\\n\\nHere, `local.dynamic_value` will return an object if the AWS region is Ireland, or the string `\\"unavailable\\"` for any other region.\\n\\n### Wait, what\'s just happened?\\n\\nInstead of using the ternary in the traditional way, we defined a tuple (aka, a list with mixed types), and used the ternary to return the index for the output we really want. Thanks mariux for the trick.\\n\\n## Less simple example\\n\\nYou can also inject dynamic outputs if needs be. I\'m not saying this is Terraform best practice, or that it will be pretty. But sometimes a woman has to do what she has to do.\\n\\nIn my case, I was trying to abstract a Step Function `Choice`. The ternary logic I needed was: if the object contains only one key, return a certain JSON structure. If the object contains multiple keys, return a different JSON structure.\\nAnd this is how my condition panned out:\\n\\n```hcl\\n# There is a reason why this variable is a list\\n# but that is beside the point now\\n\\nvariable \\"ssm_params\\" {\\n    description = \\"Parameter required by SSM Documents to enable KMS key rotation\\"\\n    default     = [{\\"KeyId\\": \\"States.Array($.KeyId)\\",\\n                    \\"AutomationAssumeRole\\": \\"States.Array($.AutomationAssumeRole)\\"}]\\n}\\n\\nlocals {\\n  choices = [for item in var.ssm_params: [\\n    # desired output if true\\n    merge(flatten([\\n      for key, value in item: {\\n        \\"IsPresent\\": true,\\n        \\"Next\\": \\"SSM-${key}\\",\\n        \\"Variable\\": \\"$.${key}\\"\\n      }\\n    ])...),\\n    # desired output if false\\n    merge([\\n      # Case: Choice step has multiple conditions (AND) \\n      { \\"And\\" : [\\n          for key, value in item: \\n            {\\n              \\"IsPresent\\": true,\\n              \\"Variable\\": \\"$.${key}\\"\\n            }\\n        ],\\n      \\"Next\\": \\"SSM-${join(\\"\\", sort([keys(item)]...))}\\"\\n      }\\n    ]...)\\n    ][\\n      # condition\\n      length(flatten([keys(item)])) == 1 ? 0 : 1\\n      ]\\n  ]\\n}\\n``` \\n\\nDon\'t ask me how my laptop haven\u2019t gone through the window. But there you have it \u2014 enjoy your newfound freedom :P"},{"id":"migrate-codecommit-to-github","metadata":{"permalink":"/blog/tech/migrate-codecommit-to-github","source":"@site/tech/2021-10-13-migrate-codecommit-to-github/index.md","title":"How to Migrate CodeCommit to GitHub \u2014 and Keep your Amplify Pipeline","description":"This tutorial includes guidance for three different scenarios in your GitHub administration:","date":"2021-10-13T00:00:00.000Z","tags":[{"label":"infra","permalink":"/blog/tech/tags/infra"},{"label":"github","permalink":"/blog/tech/tags/github"},{"label":"amplify","permalink":"/blog/tech/tags/amplify"},{"label":"aws","permalink":"/blog/tech/tags/aws"},{"label":"ci/cd","permalink":"/blog/tech/tags/ci-cd"},{"label":"tutorial","permalink":"/blog/tech/tags/tutorial"}],"readingTime":5.07,"hasTruncateMarker":true,"authors":[{"name":"Manu Magalh\xe3es","title":"DevSecOps Engineer","url":"https://github.com/magmanu","imageURL":"https://github.com/magmanu.png","key":"manu"}],"frontMatter":{"slug":"migrate-codecommit-to-github","title":"How to Migrate CodeCommit to GitHub \u2014 and Keep your Amplify Pipeline","authors":"manu","tags":["infra","github","amplify","aws","ci/cd","tutorial"]},"unlisted":false,"prevItem":{"title":"Bypassing Terraform error: \u201cThe true and false result expressions must have consistent types\u201d","permalink":"/blog/tech/terraform-ternary-error"}},"content":"This tutorial includes guidance for three different scenarios in your GitHub administration:\\n\\n1. when your repo is in your personal account;\\n2. when your app is under a GitHub Org and admins grant you the permissions you need; and\\n3. when your repo is under a GitHub Org and admins do NOT grant you the permissions you need.\\n\\n_Pre-requisites: Relevant access and permissions for CodeCommit and Amplify. You also need a working GitHub account._\\n\\n\x3c!--truncate--\x3e\\n\\n## Migrating Your Repo\\n\\n1. Open your CLI and cd into your existing CodeCommit local folder.\\n2. Run `git remote get-url origin` to get the external clone URL for the project you plan to migrate to GitHub.\\n3. Create a temporary folder by running `mkdir ../temp-migration` and open it in the CLI by running `cd ../temp-migration`.\\n4. Run `git clone --bare` followed by the clone URL you got in step 2. An example would be `git clone --bare <https://git-codecommit.eu-west-1.amazonaws.com/v1/repos/name-of-your-codecommit-directory>`.\\n\\n<br/>\\n\\n:::note Educational note\\n\\nThe flag `--bare` is a way to fully clone your repo while cutting all its ties with the remote (in CodeCommit, in this case). You still get all your branches, tags and everything, but the cloned repo is completely independent from the remote.\\n:::\\n\\n<br/>\\n\\n5. Create a new GitHub repo. To prevent issues, don\u2019t add any README, .gitignore or anything. After you click \u201cCreate repo\u201d, the only thing you should do is to copy the external clone URL as shown below. Don\u2019t run any git init, fist commit or anything. Just copy the relevant path as shown below.\\n\\n<br/>\\n\\n![Git clone SSH](./clone_ssh.webp)\\n\\n<br/>\\n\\n6. Back to your terminal, cd into your bare git repo by running `cd name-of-your-codecommit-directory.git` and run `git push --mirror` followed by the clone path you got in step 5. In my case, it will be `git push --mirror <https://github.com/my-username/my-project.git>`.\\n\\nThat\u2019s it, migration is complete now.\\n\\n### Confirm\\n\\nTo confirm all went well, go back to GitHub and refresh the page. You\u2019ll see that your repo will be there as expected, including all your commit history, branches and everything. To start working on your migrated GitHub repo, simply clone it and get on with business as usual :)\\n\\n### Clean up\\n\\nIf you followed the instructions strictly, now it\u2019s time to delete your temporary folder. Go back to your terminal and run `cd ../.. and rm -rf temp-migration`. If you used your OS\u2019s tmp folder instead, skip this step.\\n\\n## Rewiring your Amplify Pipeline\\n\\nNow that you migrated the code to GitHub, how do you leverage your already existing Amplify pipeline that was linked to the CodeCommit repo?\\n\\n### Run  the `update-app` command\\n```bash\\nAWS_PROFILE=YOUR_PROFILE AWS_DEFAULT_REGION=YOUR_REGION aws amplify update-app --app-id AMPLIFY_APP_ID --repository REPOSITORY_URL --access-token ACCESS_TOKEN\\n```\\n`AMPLIFY_APP_ID`: To find your app id, go to the Amplify console. Under App settings, click General and look for the App ARN. The app id should be the alphanumeric string at the very end of the ARN. You can find it in your Amplify console: App settings > General. The id is shown as \u201cREDACTED\u201d in the screenshot below:\\n\\n<br/>\\n\\n![Amplify App Id](./amplify_data.webp)\\n\\n<br/>\\n\\n`REPOSITORY_URL`: It\u2019s the one you got in the step 5 of the migration.\\n`ACCESS_TOKEN`: The access token is s a Personal Access Token that you can generate in GitHub. Beware that this token has to be generated by the GitHub repo owner.\\n\\n### Re-authenticate your Amplify app\\n\\n:::tip Real life XP\\n\\nCompanies can be very strict about the GitHub Apps they approve. If you get blocked because your admins, for whatever reason, won\'t approve the Amplify App, jump to the section [\\"The Webhook Approach\\"](#the-webhook-approach).\\n:::\\n\\nNow you can reconnect your app. In the same page you got your ARN, if you look above the ARN you\u2019ll find a button \u201cReconnect repository\u201d. When you click it, you\u2019ll be directed to the Oath flow in GitHub.\\n- **If the migrated repo is NOT under an Organization**, you can click the \u201cAuthorize aws-amplify-console\u201d button. You\u2019ll be redirected to the Amplify console, and once there, click again the reconnect repository, select the relevant repo and off you go! Enjoy!\\n- **If the migrated repo is under your Organization**, you can request the OAuth permission to the Amplify App by clicking a button. It will show a request pending message until your admin approves it (you\u2019ll receive an email when they do).\\n\\n<br/>\\n\\n![Authorize Amplify Github App](./authorize_amplify_app.webp)\\n\\n<br/>\\n\\nOnce Oauth is granted, go back to your Amplify app and click Reconnect repository again. You\'ll be shown repos both from your org and from your own account to choose from. Select the relevant one and have fun with your new migrated repos!\\n\\n### The Webhook Approach\\n\\nAn alternative to using the Amplify GitHub App is to create an incoming webhook in Amplify instead.\\nTo do that, under App settings, select Build settings nd click Create Webhook (both shown in orange below).\\n\\n<br/>\\n\\n![Amplify Build Settings](./amplify_build_settings.webp)\\n\\n<br/>\\n\\nIn the pop-up, type in a name and select a branch to build. Your new webhook will show in the Amplify interface. Copy its URL, we\u2019ll use it in GitHub.\\n\\n<br/>\\n\\n![Amplify Incming Webhooks](./amplify_incoming_webhooks.webp)\\n\\n<br/>\\n\\nNow go back to your migrated GitHub repo, select Settings > Webhooks and click the button Add webhook.\\n\\n<br/>\\n\\n![Adding GitHub Webhooks](./github_webhooks.webp)\\n\\nWorkaround completed.\\n\\n### Caveats of the Webhook Approach\\n\\nIf you go down the webhook path, be mindful that:\\n\\n- If you need to connect multiple branches to Amplify, you need to create a webhook in Amplify to each of them, and add each to GitHub, one by one. If you feel tempted to automate it, have a chat to your GitHub Org admin to clear up how the OAuth permission for the Amplify App can be done. Automation in this end is an overkill and waste of time.\\n- Any git push will trigger the webhook in every single frontend deployment in your Amplify. In other words, if you have the branches \u201cmain\u201d, \u201crelease\u201d, \u201cdev\u201d, \u201cfeature/a\u201d, and \u201cfeature/b\u201d, every time your colleague pushes his changes in \u201cfeature/b\u201d to GitHub, the pipeline will be triggered for all the five branches.\\n\\n\\nHope this was helpful, see you!"}]}}')}}]);